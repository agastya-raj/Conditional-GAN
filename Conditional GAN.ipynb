{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of Conditional Generative Adversarial Nets, by Mirza etal (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages and modules\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "os.chdir(\"/home/agastya/Downloads\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator input is a 4D tensor of the shape (batch_size, channels, img_dim. img_dim)\n",
    "# Generator Input is a 2D tensor of the shape (batch_size, latent_dims)\n",
    "# Labels is a 1D tensor of the shape (batch_size). It is NOT one hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200    # number of epochs of training, int, default=200\n",
    "batch_size = 64    # size of the batches, int, default=64\n",
    "lr = 0.0002    # learning rate, float, default=0.0002\n",
    "b1 = 0.5    # first order momentum gradient decay ADAM, float, default=0.5\n",
    "b2 = 0.999    # second order momentum gradient decay ADAM, float, default=0.999\n",
    "n_cpu = 8    # number of cpu threads to use during batch generation, int, default=8\n",
    "latent_dim = 100    # dimensionality of latent space, int, default=100\n",
    "img_size = 32    # size of each image dimension, int, default=28\n",
    "channels = 1    # number of image channels, int, default=1\n",
    "sample_interval = 400    # interval between image samples, int, default=400\n",
    "n_classes = 10    # number of classes for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (channels, img_size, img_size)\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.0 Data Preparation and Preprocessing\n",
    "# Use transforms.Resize(img_size) to resize the image into (batch_size, 1, img_size, img_size)\n",
    "def mnist_data():\n",
    "    compose = transforms.Compose([\n",
    "         transforms.Resize(img_size),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((.5, .5, .5), (.5, .5, .5)) \n",
    "         #Normalized to (-1,1) so as to mimic a tanh activation function\n",
    "        ])\n",
    "    out_dir = './dataset'\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "data = mnist_data()\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights initialization\n",
    "# gaussian Distribution works the best\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm1d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm with a momentum of 0.8\n",
    "# The desired class is embedded into an extra tensor\n",
    "# An extra tensor is concatenated with the input image\n",
    "# Linear GAN is used instead of DCGAN\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, input_dims, output_dims, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.negative_slope = 0.2\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims= output_dims\n",
    "        self.img_shape = img_shape\n",
    "        \n",
    "        self.label_embeds = nn.Embedding(n_classes, n_classes)\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dims+n_classes, 128)\n",
    "        self.layer2 = nn.Linear(128, 256)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(256, 0.8)\n",
    "        self.layer3 = nn.Linear(256, 512)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(512, 0.8)\n",
    "        self.layer4 = nn.Linear(512, 1024)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(1024, 0.8)\n",
    "        self.layer5 = nn.Linear(1024, output_dims)\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        \n",
    "        embeds = self.label_embeds(labels)\n",
    "        x = torch.cat((x, embeds), -1)\n",
    "        x = F.leaky_relu_(self.layer1(x), self.negative_slope)\n",
    "        x = F.leaky_relu_(self.batchnorm2(self.layer2(x)), self.negative_slope)\n",
    "        x = F.leaky_relu_(self.batchnorm3(self.layer3(x)), self.negative_slope)\n",
    "        x = F.leaky_relu_(self.batchnorm4(self.layer4(x)), self.negative_slope)\n",
    "        x = F.tanh(self.layer5(x))\n",
    "        x = x.view(x.size(0), *self.img_shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8a5942d806a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# number of classes is embedded into a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Leaky RELU rules!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# number of classes is embedded into a tensor\n",
    "# Leaky RELU rules!!\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, input_dims, dropout=0.4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.label_embeds = nn.Embedding(n_classes, n_classes)\n",
    "        self.negative_slope = 0.2\n",
    "        \n",
    "        self.layer1 = nn.Linear(n_classes+input_dims, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer3 = nn.Linear(512, 512)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.layer4 = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        embeds = self.label_embeds(labels)\n",
    "        x = torch.cat((x.view(x.size(0), -1), embeds), -1)\n",
    "        x = F.leaky_relu_(self.layer1(x), self.negative_slope)\n",
    "        x = F.leaky_relu_(self.dropout2(self.layer2(x)), self.negative_slope)\n",
    "        x = F.leaky_relu_(self.dropout3(self.layer3(x)), self.negative_slope)\n",
    "        x = F.sigmoid(self.layer4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary CrossEntropy Loss\n",
    "# For discriminator, maximize log(D(x)) + log (1 - D(G(z)))\n",
    "# For generator, maximize log(D(G(z)))\n",
    "adversarial_loss = nn.BCELoss()\n",
    "generator = Generator(n_classes, latent_dim, img_size**2, img_shape)\n",
    "discriminator = Discriminator(n_classes, img_size**2)\n",
    "generator.apply(init_weights)\n",
    "discriminator.apply(init_weights)\n",
    "gen_optim = optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "disc_optim = optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random noise froma Gaussian Distribution\n",
    "def noise(size):\n",
    "    n = torch.tensor(torch.randn(size, 100))\n",
    "    if torch.cuda.is_available():\n",
    "        return n.cuda()\n",
    "    else:\n",
    "        return n\n",
    "\n",
    "# Returns an array of 1's for real data\n",
    "def real_data_targets(size):\n",
    "    data = torch.tensor(torch.ones(size, 1))\n",
    "    if torch.cuda.is_available():\n",
    "        return data.cuda()\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Returns a array of 0's for fake data\n",
    "def fake_data_targets(size):\n",
    "    data = torch.tensor(torch.zeros(size, 1))\n",
    "    if torch.cuda.is_available():\n",
    "        return data.cuda()\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Randomized data classes from 0-10 from a uniform distribution\n",
    "def fake_data_labels(size):\n",
    "    labels = torch.tensor(torch.randint(low=0,high=10, size=(1,size)).view(-1), dtype=torch.long)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_disc(optimizer, real_data, real_labels, gen_data, gen_labels):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Training on real data\n",
    "    # real_loss = -log(D(x))\n",
    "    real_prediction = discriminator(real_data, real_labels)\n",
    "    real_loss = adversarial_loss(real_prediction, real_data_targets(real_data.size(0)))\n",
    "    real_loss.backward()\n",
    "    \n",
    "    # Training on generated data\n",
    "    # fake_loss = -log(1 - D(G(z)))\n",
    "    fake_prediction = discriminator(gen_data, gen_labels)\n",
    "    fake_loss = adversarial_loss(fake_prediction, fake_data_targets(gen_data.size(0)))\n",
    "    fake_loss.backward()\n",
    "    \n",
    "    # Actual gradient update\n",
    "    # Total_loss = -log(D(x)) - log(1 - D(G(z))\n",
    "    optimizer.step()\n",
    "    \n",
    "    return real_loss+fake_loss, real_prediction, fake_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen(optimizer, fake_data, fake_labels):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Generator loss is -log(D(G(z)))\n",
    "    prediction = discriminator(fake_data, fake_labels)\n",
    "    loss = adversarial_loss(prediction, real_data_targets(fake_data.size(0)))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GAN(num_epochs):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (real_data, real_labels) in enumerate(data_loader):\n",
    "            \n",
    "            real_data = torch.tensor(real_data.view(real_data.size(0),img_size**2))\n",
    "            real_labels = real_labels\n",
    "            if torch.cuda.is_available():\n",
    "                real_data = real_data.cuda()\n",
    "            fake_labels = fake_data_labels(real_data.size(0))\n",
    "            fake_data = generator(noise(real_data.size(0)), fake_labels).detach() # Don't train generator when training discriminator\n",
    "            \n",
    "            # First train Discriminator without training discriminator\n",
    "            disc_error, disc_real_pred, disc_fake_pred = train_disc(disc_optim, real_data, real_labels, fake_data, fake_labels)\n",
    "            \n",
    "            # Then train generator via discriminator without training discriminator\n",
    "            # that's why two optimizers with only each parameters of discriminator and generator\n",
    "            fake_data = generator(noise(real_data.size(0)), fake_labels)\n",
    "            gen_error = train_gen(gen_optim, fake_data, fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-fa8a13970fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_GAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-84-82cc8b275053>\u001b[0m in \u001b[0;36mtrain_GAN\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mfake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mgen_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-8f93531d7b36>\u001b[0m in \u001b[0;36mtrain_gen\u001b[0;34m(optimizer, fake_data, fake_labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_data_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cpu1.8/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cpu1.8/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_GAN(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
